{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import opendatasets as od\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download('https://www.kaggle.com/datasets/yaswanthgali/dog-images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_urls(root_folder=\"dog-images/images/images\"):\n",
    "    image_urls = []\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp')\n",
    "\n",
    "    for folder_path, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                relative_path = os.path.join(folder_path, file)\n",
    "                url = f\"/{relative_path.replace(os.sep, '/')}\"\n",
    "                image_urls.append(url)\n",
    "\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square(img):\n",
    "    width, height = img.size\n",
    "    if width == height:\n",
    "        return img\n",
    "    elif width > height:\n",
    "        result = Image.new(img.mode, (width, width), (0, 0, 0))\n",
    "        result.paste(img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(img.mode, (height, height), (0, 0, 0))\n",
    "        result.paste(img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n",
    "class VAEDataset(Dataset):\n",
    "    def __init__(self, image_urls, transform=None):\n",
    "        self.image_urls = image_urls\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Lambda(pad_to_square),\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_urls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_urls[idx].lstrip('/')  # Remove leading slash\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "def get_vae_dataloader(image_urls, batch_size=32, shuffle=True, num_workers=4):\n",
    "    dataset = VAEDataset(image_urls)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = get_image_urls()\n",
    "dataset = VAEDataset(image_urls)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256 * 32 * 32, latent_dim)\n",
    "        self.fc_var = nn.Linear(256 * 32 * 32, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256 * 32 * 32)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 256, 32, 32)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        recon_x, mu, log_var = self(x)\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum') * 512 * 512 * 3\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + kl_div\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    def on_train_epoch_end(self):\n",
    "        # Test the model at the end of each epoch\n",
    "        self.test_model()\n",
    "\n",
    "    def test_model(self):\n",
    "        # Get a batch of images from the training data\n",
    "        batch = next(iter(self.trainer.train_dataloader))\n",
    "        self.visualize_results(batch, f\"Epoch {self.current_epoch}\")\n",
    "\n",
    "    def visualize_results(self, batch, title_prefix):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Test with images from the dataset\n",
    "            img = batch[:2].to(self.device)  # Take up to 2 images\n",
    "            recon, _, _ = self(img)\n",
    "            for i in range(len(img)):\n",
    "                original = img[i].cpu().permute(1, 2, 0).numpy()\n",
    "                reconstructed = recon[i].cpu().permute(1, 2, 0).numpy()\n",
    "                self.plot_results(original, reconstructed, f\"{title_prefix} - Sample {i+1} from Dataset\")\n",
    "            \n",
    "            # Test with random noise\n",
    "            noise = torch.randn(2, self.latent_dim).to(self.device)\n",
    "            generated = self.decode(noise)\n",
    "            for i in range(2):\n",
    "                generated_img = generated[i].cpu().permute(1, 2, 0).numpy()\n",
    "                self.plot_results(np.zeros_like(generated_img), generated_img, f\"{title_prefix} - Generated from Random Noise {i+1}\")\n",
    "            \n",
    "            # Interpolation in latent space\n",
    "            img1, img2 = batch[:2]  # Get two images\n",
    "            img1, img2 = img1.to(self.device), img2.to(self.device)\n",
    "            mu1, _ = self.encode(img1.unsqueeze(0))\n",
    "            mu2, _ = self.encode(img2.unsqueeze(0))\n",
    "            interpolations = torch.zeros(7, self.latent_dim).to(self.device)\n",
    "            for i, alpha in enumerate(np.linspace(0, 1, 7)):\n",
    "                interpolations[i] = alpha * mu1 + (1 - alpha) * mu2\n",
    "            generated = self.decode(interpolations)\n",
    "            self.plot_interpolation(generated, f\"{title_prefix} - Interpolation in Latent Space\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def plot_results(self, original, reconstructed, title):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        ax1.imshow(original)\n",
    "        ax1.set_title('Original')\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(reconstructed)\n",
    "        ax2.set_title('Reconstructed')\n",
    "        ax2.axis('off')\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    def plot_interpolation(self, generated, title):\n",
    "        fig, axes = plt.subplots(1, 7, figsize=(20, 4))\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(generated[i].cpu().permute(1, 2, 0).numpy())\n",
    "            ax.axis('off')\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = VAE()\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator=\"auto\", devices=\"auto\", strategy=\"auto\", accumulate_grad_batches=4, precision=\"bf16-mixed\")\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "torch.save(model.state_dict(), 'vae_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated images are not great likely due to the extreme variation in the dataset"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1767097,
     "sourceId": 2884709,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
